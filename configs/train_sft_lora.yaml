# configs/train_sft_lora.yaml
base_model: "meta-llama/Meta-Llama-3-8B-Instruct"
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lr: 1e-4
epochs: 1
per_device_train_batch_size: 1
max_seq_len: 2048
